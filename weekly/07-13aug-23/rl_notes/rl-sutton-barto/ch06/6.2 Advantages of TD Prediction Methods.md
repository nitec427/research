# Ch6.2 Advantages of TD Prediction Methods

TD methods update their estimates based in part on other estimates. They learn a guess from a guess (******************bootstrap******************). 

Obvious advantage of TD methods over MC methods is that they are implemented online and fully incremental fashion. 

With MC methods to obtain the return you must wait until the end of an episode, because only then the return is known.

Whereas with TD methods you only wait one time step.  Surprisingly often this is a critical consideration. 

Some episodes take too long to finish, and delaying all learning until the end of episode is too slow.

Other applications are continuing tasks and have no episodes at all. 

Finally, some MC methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning. 

******************But are TD methods sound? Can we still guarantee its convergence to the correct answer?******************

Yes, for an y fixed policy pi, TD(0) has been proved to converge $v_\pi$, in the mean for a constant step-size parameter if it is sufficiently small, and with probility 1 if the step-size parameter decreases according to the usual stochastic approximation condtions.

If both TD and MC converge to the correct predictions, ************which gets there first. At the current time this is an open question************ and no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question.