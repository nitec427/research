# Off-Policy Prediction via Importance Sampling

How can you learn about optimal policy, while you behave according to an exploratory policy?

The on-policy approach in previous part is actually a compromise, learn action values for near-optimal policy that still explores.

A more straightforward approach is to use two policies, one that is learned about (****************************target policy)**************************** and becomes optimal policy, and one that is more exploratory and used to generate behavior (**********************behavior policy)**********************.

In this case we say that learning is from data ********************************************off the target policy, and the overall process is termed off-policy learning.********************************************

The rest of the book will be both about on and off-policy. Off-policy methods are harder than on-policy methods. However they also contain on-policy methods. They are more powerful and general compared to on-policy methods.

Because the data is due to a different policy, off-policy methods are often of greater variance and slower to converge.

We begin the study of off-policy methods by considering **********************prediction problem.********************** In which both the target and behavior policies are fixed. Suppose we wish to estimate $v_\pi$ or $q_\pi$, but all we have episodes of following another policy.