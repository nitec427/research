# Monte Carlo Without Exploring Starts

We need to avoid unlikely assumption of ES.

**Exploring Starts yöntemi oldukça düşük ihtimalle yeni bir aksiyon seçer ve biz bunu bekleyemeyiz.**

The only general way for an agent to ensure select all actions, it must select infinitely often.

**Bütün aksiyonların seçildiğine emin olmamız için agent sonsuz kere seçim yapmalı.**

There are two approaches **************************on-policy and off-policy methods.**************************

**Bu aksiyon seçimini yapmak için on-policy ya da off-policy (ayrık-bağımsız politika) yöntemleri mevcut.**

On-policy methods attempt to improve or evaluate a policy that is used to make decisions.

**********************On-policy (üzerinde politika) yöntemleri eldeki karar veren politikayı iyileştirmeye ve değerlendirmeye çalışır.********************** 

Off-policy methods methods evaluate or improve a policy different from that used to generate the data.

****Off-policy (bağımsız politika, ayrık) ise veriyi üreten politikadan bağımsız olarak farklı bir politikayı iyileştirir.****

Monte Carlo exploring starts (****ES****) is an on-policy method, to mention. 

The reason we avoid exploring starts is that the node (state) selection is unlikely for even large numbers. (**********************************************unrealistic assumption)**********************************************

In on-policy control methods the policy is generally soft, meaning $\pi(a|s)$ > 0 for all a and s possible.

Üzerinde politikayla kontrol yöntemlerinde elimizdeki politika **************yumuşak ve esnektir, yani herhangi bir politika ve evre ile aksiyonların gelme olasılığı 0dan büyüktür.**************

The on-policy method presented in this section is ********************e-greedy.******************** 

Burada gösterilecek olan algoritma e-greedy yöntemidir.

Most of the time you pick the greedy action with probability 1 - e + e / |A(s)|, for other A-1 action you follow the remaining |A| - 1 actions with probability e / |A(s)|

e-greedy policies are examples of e-soft policies, defined as policies for which

 $\pi(a|s) >= \Large \frac {\epsilon}{|A(s)|}$

As in Monte Carlo ES, we use first-visit MC methods to estimate action-value function for current policy.

For any e-soft policy pi, any e-greedy policy with respect $q_\pi$ is guaranteed to be better than or equal to $\pi$.

That any e-greedy policy with respect to q_pi is an improvement over any e-soft policy pi assured by the policy improvement theorem.

Let $\pi'$ be the e-greedy policy. The conditions of the policy improvement theorem apply because for any $s \in S$